---
title: "Student Performance Data Analysis"
author: "Alyssa Rogers-Armstrong"
date: "June 19th, 2024"
output: pdf_document
---
## Data Cleaning

```{r}
#install.packages("")
library(ggplot2) #plots
library(car)
library(MASS)
library(knitr)
library(formatR)
library(reshape2) #melt plots together
library(olsrr) # model selection
knitr::opts_chunk$set(tidy.opts = list(width.cutoff = 60), tidy = TRUE)

#Read in the data
setwd("/Users/alyssarogers-armstrong/Documents/Virginia Tech/Summer 2024/Advanced Methods of Regression/Exams/Final Project")
Data = read.csv("studentperformance.csv")
View(Data)

#Check the classes of the variables and change to factors for categorical variables
Data$class_period = as.factor(Data$class_period)
Data$ell_level = as.factor(Data$ell_level)
Data$ethnicity = as.factor(Data$ethnicity)
Data$gender = as.factor(Data$gender)
Data$honors_math = as.factor(Data$honors_math)
str(Data)
```

## Exploratory Data Analysis

```{r}
melt_dat = melt(Data, "sol_score")

ggplot(melt_dat, aes(x=value, y=sol_score)) +
  geom_point() +
  facet_wrap(~ variable, scales = "free") +
  theme_bw()

ggplot(Data, aes(y=sol_score, x=class_period))+geom_point()+theme_bw()

aggregate(sol_score ~ class_period, data = Data, mean)
```

- Positive relationship (average SOL score increases as variable increases) between average map score, average grades, dessa, honors math, and test average.


```{r}

# Full Model
fullmodel = lm(sol_score ~ class_period + ell_level  + test_averages + average_map + average_grades + ethnicity + gender + absences + tardys +  ixl_skills_practiced + dessa, data=Data)
summary(fullmodel)

library(car)
vif(fullmodel)


# Install and load required packages
library(ggcorrplot)

# Select numeric columns from the dataset
numeric_data <- Data[, sapply(Data, is.numeric)]

# Calculate the correlation matrix
cor_matrix <- cor(numeric_data, use = "complete.obs")

# Create the heatmap
ggcorrplot(cor_matrix, method = "circle", type = "lower",
           lab = TRUE, lab_size = 3, title = "Correlation Matrix Heatmap",
           colors = c("blue", "white", "red"))

#Use this code if I want to use a different factor level as reference
#lm(x ~ y + relevel(b, ref = "3")) 
```

```{r}
# Forward Selection
forwardmodel = ols_step_forward_aic(fullmodel)
forwardmodel
```

```{r}
# Backward Selection
backwardmodel = ols_step_backward_aic(fullmodel)
backwardmodel
```

```{r}
# Stepwise Selection
stepwisemodel = ols_step_both_aic(fullmodel)
stepwisemodel
```

**Forward + Stepwise + Backward:**

- All of the selection processes included the same variables in their models.

    + Average MAP Score
    
    + Test Average
    
    + DESSA
    
    + Class Period
    
- AIC: 912.597

- Adjusted R-Squared: 0.761

- MSE: 488.568

- RMSE: 21.201



**Model:**

- Because all of the selection processes produced the same model I will go with that one.

```{r}
finalmodel = lm(sol_score ~ class_period + dessa + average_map + test_averages, data=Data)
summary(finalmodel)
```
**NEED TO UPDATE THESE INTERPRETATIONS**

- **Average MAP Score**: For one unit increase in student's Average MAP Score, the SOL Score will increase 2.0689 units on average, holding the other predictors constant.

- **Average Math Test Score**: For one unit increase in student's Average Math Test Score, the SOL Score will increase 0.8770 units on average, holding the other predictors constant.

- **Class Period 2**: Being in Class Period 2 is associated with a 20.0925 decrease in the SOL Score on average, compared to being in Class Period 1, holding all other variables constant.

- **Class Period 6**: Being in Class Period 6 is associated with a 21.74 decrease in the SOL Score on average, compared to being in Class Period 1, holding all other variables constant

    + This estimate seems incorrect compared the scatterplot that shows that period 6 scores higher on the SOL. I may need to check assumptions and do a transformation.

- **Class Period 7**: Being in Class Period 7 is associated with a 15.845 decrease in the SOL Score on average, compared to being in Class Period 1, holding all other variables constant.

- **ELL Level 4**: Being an ELL Level 4 is associated with a 28.155 increase in SOL Score on average, compared to being an ELL Level 0, holding all other variables constant.


## Confidence Intervals

```{r}
CI = round(confint(finalmodel),3)
knitr::kable(CI)
```

- The confidence intervals are very wide which raises some suspicions with the model. I will check assumptions next.

## Assumptions

### Normal Probability Plot

```{r}
Data$fitted = finalmodel$fitted.values #adding fitted values
Data$residuals = finalmodel$residuals #adding residuals
Data$studres = studres(finalmodel) #adding studentized residuals

# Regular Residuals
ols_plot_resid_qq(finalmodel)

# Studentized Residuals
qqPlot(finalmodel, ylab="Studentized Residuals", xlab="Theoretical Quantiles")
```

- Not all of the data falls on the 45 degree line, but I don't think this is too suspicious

### Residuals vs. Predicted

```{r}
# Regular Residuals
ols_plot_resid_fit(finalmodel)

# Studentized Residuals
ggplot(data=Data, aes(x=fitted, y=studres))+geom_point()+
  labs(y="Studentized Residuals", x="Fitted Values")+
  geom_hline(yintercept=0, color='red')+theme_bw()

plot(finalmodel, which = 3)
```

- It looks like there might be some funneling in these plots

- We might need to use a log transformation

- This is a non-constant variance problem.

### Residuals vs. Regressors in the Model

```{r}
# Create Melted Dataset
ind = which(names(Data)%in%names(finalmodel$coefficients))

# Regular Residuals
melt_regresid = melt(Data[c(ind,16)], "residuals")

# Studentized Residuals
melt_studresid = melt(Data[c(ind,17)], "studres")

# Regular Residual plot
ggplot(melt_regresid, aes(x=value, y=residuals)) +
  geom_point() + facet_wrap(~ variable, scales = "free") +
  theme_bw()

# Studentized Residual Plot
ggplot(melt_studresid, aes(x=value, y=studres)) +
  geom_point() + theme_bw()+
  facet_wrap(~ variable, scales="free") +
  labs(y="Studentized Residuals")
```

- Test average seems to have a funnel effect in both the regular residual plot and the studentized residual plot

# Transformation

```{r}
p=ggplot(data=Data,aes(x=test_averages,y=sol_score))
p+geom_point()+theme_bw()+scale_y_continuous(trans='log',
breaks=c(0.1,2.5,5,7.5))

ggplot(data=Data,aes(x=test_averages,y=log(sol_score)))+geom_point()+theme_bw()
```

```{r}
ModelTran=lm(log(sol_score)~ class_period + average_map + dessa + test_averages, Data)
summary(ModelTran)
```

```{r}
Data$LogStudRes=studres(ModelTran)
Data$LogFitted=ModelTran$fitted.values
Data$LogRes = ModelTran$residuals 

ggplot(data=Data,aes(x=LogFitted,y=LogStudRes))+geom_point()+
labs(y="Studentized Residuals",x="Fitted Values (Log Model)")+
geom_hline(yintercept=0,color='red')+theme_bw()


# Create Melted Dataset
ind1 = which(names(Data)%in%names(ModelTran$coefficients))

# Regular Residuals
melt_regresid1 = melt(Data[c(ind1,20)], "LogRes")

# Studentized Residuals
melt_studresid1 = melt(Data[c(ind1,18)], "LogStudRes")

# Regular Residual plot
ggplot(melt_regresid1, aes(x=value, y=LogRes)) +
  geom_point() + facet_wrap(~ variable, scales = "free") +
  theme_bw()

# Studentized Residual Plot
ggplot(melt_studresid1, aes(x=value, y=LogStudRes)) +
  geom_point() + theme_bw()+
  facet_wrap(~ variable, scales="free") +
  labs(y="Studentized Residuals")
```

```{r}
library(MASS)
boxcox(finalmodel)
```

## Multicollinearity

### Create a scatterplot matrix with correlations

```{r}
terms = names(finalmodel$coefficients)
ind=which(names(Data)%in%terms)
library(psych)

pairs.panels(Data[ind],
             method = "pearson",
             hist.col = "darkorange",
             density = TRUE,
             ellipses = TRUE)
```

- There are no collinearity values > .80 in absolute value.

```{r}
library(olsrr)
knitr::kable(ols_vif_tol(finalmodel))
```

- There are no VIF values > 10.

```{r}
library(kableExtra)
knitr::kable(round(ols_eigen_cindex(finalmodel),3),format='latex',booktabs=TRUE)%>%
  kable_styling(latex_options="scale_down")

ols_coll_diag(finalmodel)


```

- There are no condition indices that are > 30.

## Check for Polynomial Variables

- Adding a polynomial term on the average_map variable made the model more complex and didn't help the assumptions at all. It also made the VIF scores worse.

```{r}
library(car)
avPlots(finalmodel)

Data$average_map.sq = Data$average_map^2
modelpolynomial = lm(sol_score ~ class_period + dessa + average_map + average_map.sq + test_averages, data=Data)
knitr::kable(round(summary(modelpolynomial)$coefficients,3))

avPlots(modelpolynomial)

knitr::kable(ols_vif_tol(modelpolynomial))

ols_plot_resid_qq(modelpolynomial)
```

## Interaction Model

```{r}
#Interaction Model
interaction_model <- lm(sol_score ~ class_period * (dessa + average_map + test_averages), data = Data)
summary(interaction_model)

# Fit the model with the interaction term
interaction_model <- lm(sol_score ~ class_period + average_map +
                        class_period:test_averages, data = Data)
summary(interaction_model)

full_interaction_model = lm(sol_score ~ class_period + ell_level + test_averages + average_map + average_grades + ethnicity + gender + absences + tardys + ixl_skills_practiced + dessa + class_period*test_averages, data=Data)

stepinteraction = ols_step_both_aic(full_interaction_model)
stepinteraction


# Example: Visualize interaction between class_period and average_map if significant
ggplot(Data, aes(x = average_map, y = sol_score, color = class_period)) +
  geom_point() +
  geom_smooth(method = "lm", se = FALSE) +
  labs(title = "Interaction between Class Period and Average MAP Score",
       x = "Average MAP Score",
       y = "SOL Score",
       color = "Class Period") +
  theme_minimal()

#avPlots(interaction_model)


knitr::kable(ols_vif_tol(interaction_model))

ols_plot_resid_qq(interaction_model)


# Center the average_map variable
Data$average_map_centered <- Data$average_map - mean(Data$average_map, na.rm = TRUE)

# Fit the model with the centered interaction term
interaction_model_centered <- lm(sol_score ~ class_period * average_map_centered + dessa + test_averages, data = Data)

# Check VIF scores for main effects
library(car)
vif(interaction_model_centered, type = "predictor")

summary(interaction_model_centered)

knitr::kable(ols_vif_tol(interaction_model_centered))

ols_plot_resid_qq(interaction_model_centered)

plot(interaction_model_centered, which = 3)

avPlots(interaction_model_centered)

ols_plot_resid_fit(interaction_model_centered)

```


## Calculate Leverage and Outliers from centered interaction model

- This model seems to be robust to outliers

```{r}
ols_plot_resid_lev(interaction_model_centered)
```

```{r}
ols_plot_cooksd_chart(interaction_model_centered)
```

```{r}
ols_plot_dffits(interaction_model_centered)
```

```{r}
ols_plot_dfbetas(interaction_model_centered)
```

```{r}
ind=c(52)
datareduced=Data[-ind,]

modelreduced = lm(sol_score ~ class_period * average_map_centered + dessa + test_averages, data = datareduced)

summary(interaction_model_centered)
summary(modelreduced)
```
    